# ES集群规划

## 集群节点类型
**Master Node（主节点）**：  
一个集群只有一个Master节点，Master节点不接受客户端请求，它主要用来控制集群的操作（创建/删除索引、分片/副本的分配）和控制集群状态。只有Master Node节点可以修改节点状态信息及元数据(metadata)的处理，比如索引的新增、删除、分片路由分配、所有索引和相关 Mapping 、Setting 配置等等

**Master Eligibel Node（候选主节点）**：  
默认情况下，所有节点一启动就是候选主节点，可以参加选主流程。它只是与集群保持心跳，判断Master是否存活，如果Master故障则参加新一轮的Master选举。若不想成为候选主节点，可以设置node.master: false。

**Data Node（数据节点）**：  
负责数据的存储和相关的操作，例如对数据进行增、删、改、查和聚合等操作，所以数据节点（Data 节点）对机器配置要求比较高，对 CPU、内存和 I/O 的消耗很大。默认，所有节点都是Data Node，可以设置参数node.data: false来使其不是Data Node。
    
**Coordinating Node（协调节点）**：  
协调节点会检索应用创建连接、接受检索请求，但其本身不负责存储数据，可看做是负载均衡节点，该节点不占用io、cpu和内存资源。协调节点接受客户端搜索请求后将请求转发到与查询条件相关的多个Data Node的分片上，然后多个Data Node的分片执行查询语句或者查询结果再返回给协调节点，协调节点把各个Data Node的返回结果进行整合 、排序等一系列操作后再将最终结果返回给用户请求。默认，每一台ES实例都是Coordinatiing Node。

**Ingest Node（摄取节点）**：  
可以看作是数据前置处理转换的节点，对数据进行过滤、转换等操作，类似于Logstash中filter的作用，功能相当强大。默认情况下，所有节点都启用Ingest，因此任何节点都可以处理Ingest任务。

## 节点、分片规划
**Master Node**：  
ES集群，Master/Master Eligibel节点至少三台服务器，三个Master节点最多只能故障一台Master节点，数据不会丢失，如果三个节点故障两个节点，则造成数据丢失并无法组成集群。

**Data Node**：  
在ES集群中，此节点应该是最多的，单个索引在一个Data Node实例上分片数保持在3个以内(分片数量按照Data节点数量划分比较好，每个节点上存储一个分片)，每1GB堆内存对应集群的分片保持在20个以内
Coordinating Node: 增加协调节点可增加检索并发，但检索的速度还是取决于查询所命中的分片个数以及分片中的数据量。

**Data Node内存**：  
假如一台机器部署了一个ES实例，则ES最大可用内存给到物理内存的50%，最多不可超过32G，如果单台机器上部署了多个ES实例，则多个ES实例内存相加等于物理内存的50%，多个ES实例内存相加不宜超过32G。

**分片**：  
官方建议日志类应用，单个分片不要大于50G，搜索类应用，单个分片不要大于20G

## 避免分片不均衡
ES分片策略会尽量保证节点上的分片数大致相同，当新增加Data Node后，新索引会集中存储到新加节点上，这肯定会有问题。  
可以设定如下参数来避免此类问题：
```bash
index.routing.allocation.total_shards_per_node  #单个节点上，针对每个索引分配的最大分片数（副本和主副本都包含）。默认为无限。
```
还可以限制节点可以拥有的分片数量，而与索引无关：
```bash
cluster.routing.allocation.total_shards_per_node  #（动态）分配给每个节点的主要和副本分片的最大数量（所有索引）。默认为 -1（无限制）。
```
ES在分片分配期间检查此设置。  
示例：假如一个集群的 cluster.routing.allocation.total_shards_per_node 设置为 100 ，三个节点具有以下分片分配现状：  
- 节点 A ：100 个分片
- 节点 B ：98 个分片
- 节点 C ：1 个分片  

如果节点 C 发生故障时，节点C上的那1个分片只能重新分配给节点 B，因为A节点已经满了。

## 容量规划
考虑要素：
- 一个集群总共需要多少个节点？
- 一个索引需要设置几个分片？
- 要保持一定的余量，当负载出现波动，某个节点离线，要保证整个集群数据正常
- 机器软硬件配置
- 单条文档尺寸、文档总数据量、索引总数据量

硬件配置：
- 数据节点尽可能使用SSD
- 搜索等性能要求高的场景，使用SSD，内存：硬盘比例1:10，比如32G内存，320G SSD硬盘
- 日志类和查询并发低的场景，使用机械硬盘，内存：硬盘比例1:50
- 单节点数据建议控制在2T以内
- JVM配置机器内存总量的1/2，JVM内存配置建议不超过32G

内存设定最佳实践：
- 内存大小要根据Node需要存储的数据大小来进行估算
    - 搜索类比例建议：1:16
    - 日志类比例建议：1:48 - 1:96之间
    
    示例：
    -  数据1T，设置一个副本，总共消耗磁盘存储2T
        - 搜索类，假设每个节点总内存64G，设定JVM内存32G，那么单个节点最多支撑32*16=512G，系统需要额外留出富余，所以磁盘容量要比512G多才可以。而真正需要消耗2T，那么需要4个Data Node。
        - 日志类，假设每个节点总内存64G，设定JVM内存32G，那么单个节点最多支撑32*50=1600G。数据总量为2T，所以两个Data Node足够了。

